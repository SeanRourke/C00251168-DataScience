{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Flower Classification Convolutional Neural Network Project\n",
    "\n",
    "### Dataset\n",
    "\n",
    "Dataset containing 4,242 images of flowers. Dataset obtained from Kaggle at https://www.kaggle.com/datasets/alxmamaev/flowers-recognition. \n",
    "\n",
    "Dataset contained following (class / number of images):\n",
    "\n",
    "- daisy / 764\n",
    "- dandelion / 1,052\n",
    "- rose / 784\n",
    "- sunflower / 733\n",
    "- tulip / 984\n",
    "\n",
    "##### Preprocessing\n",
    "\n",
    "All images are resized to 244 x 244 in this notebook.\n",
    "\n",
    "### Findings\n",
    "\n",
    "From performing operations on this dataset, I have found that this convolutional neural network created to classify the flowers was not accurate. As seen by the accuracy score of around 0.60, the model is correct in its classification more often than not, but is not consistent enough to be relied upon."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Changelog\n",
    "\n",
    "#### Version 1\n",
    "\n",
    "- Set root directory for image locations\n",
    "- Read in images.\n",
    "- Resized images to all be the same size.\n",
    "- Create ImageDataGenerator.\n",
    "- Split data into training and validation data.\n",
    "- Created  and compiled model.\n",
    "- Trained model on the data.\n",
    "- Analysed accuracy of the model using accuracy and loss.\n",
    "\n",
    "#### Version 2\n",
    "\n",
    "- Tried multiple different numbers for batch size and epochs\n",
    "- Compared results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import all necessary libraries\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "import cv2\n",
    "import os\n",
    "from tensorflow.keras import layers, models\n",
    "import numpy as np\n",
    "from tensorflow.keras.preprocessing import image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set root directory for class folders and target size for images\n",
    "\n",
    "root_directory = 'flowers/'\n",
    "target_size = (224, 224)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Resize images in dataset to target_size\n",
    "\n",
    "for class_folder in os.listdir(root_directory):\n",
    "    class_path = os.path.join(root_directory, class_folder)\n",
    "    if os.path.isdir(class_path):  # Check if it is a directory\n",
    "        # Loop through all images in the class folder\n",
    "        for filename in os.listdir(class_path):\n",
    "            if filename.endswith('.jpg') or filename.endswith('.png'):\n",
    "                img_path = os.path.join(class_path, filename)\n",
    "                img = cv2.imread(img_path)\n",
    "                img_resized = cv2.resize(img, target_size)\n",
    "                cv2.imwrite(img_path, img_resized)  # Overwrite the original image or save to a new file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set number of training samples used in one iteration of training\n",
    "# Set number of passes through dataset\n",
    "\n",
    "batch_size = 32\n",
    "epochs = 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create ImageDataGenerator to allow real-time data augmentation. This helps the model generalise and reduces overfitting\n",
    "# Rescale pixel values from 0-255 to 0-1 to improve convergence during training\n",
    "# Reserve 20% of the dataset for validation\n",
    "\n",
    "train_datagen = ImageDataGenerator(rescale=1./255, validation_split=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 3457 images belonging to 5 classes.\n"
     ]
    }
   ],
   "source": [
    "# Load training data\n",
    "\n",
    "train_generator = train_datagen.flow_from_directory(\n",
    "    root_directory,\n",
    "    target_size=target_size,\n",
    "    batch_size=batch_size,\n",
    "    class_mode='categorical',\n",
    "    subset='training'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'daisy': 0, 'dandelion': 1, 'rose': 2, 'sunflower': 3, 'tulip': 4}\n"
     ]
    }
   ],
   "source": [
    "# Check class indice values\n",
    "\n",
    "print(train_generator.class_indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 860 images belonging to 5 classes.\n"
     ]
    }
   ],
   "source": [
    "# Load validation data\n",
    "\n",
    "validation_generator = train_datagen.flow_from_directory(\n",
    "    root_directory,\n",
    "    target_size=target_size,\n",
    "    batch_size=batch_size,\n",
    "    class_mode='categorical',\n",
    "    subset='validation'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Three 2D convolution layers for RGB colours, filter size of 3x3\n",
    "# Reduce height and width of feature maps to reduce number of parameters and computation in the network\n",
    "\n",
    "model = models.Sequential([\n",
    "    layers.Conv2D(32, (3, 3), activation='relu', input_shape=(224, 224, 3)),\n",
    "    layers.MaxPooling2D(pool_size=(2, 2)),\n",
    "    layers.Conv2D(64, (3, 3), activation='relu'),\n",
    "    layers.MaxPooling2D(pool_size=(2, 2)),\n",
    "    layers.Conv2D(128, (3, 3), activation='relu'),\n",
    "    layers.MaxPooling2D(pool_size=(2, 2)),\n",
    "    layers.Flatten(),   # flatten 3D output to 1D vector for following layers\n",
    "    layers.Dense(128, activation='relu'),   # Learn high-level features by combining features learning in convolutional layers\n",
    "    layers.Dense(len(train_generator.class_indices), activation='softmax')  # Same number of neurons as classes, softmax outputs probability for each class\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adaptive Moment Estimation optimiser because it is memory-efficient and adapts the learning rate dynamically for each parameter\n",
    "# Minimise loss function during training\n",
    "\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Seán\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\keras\\src\\trainers\\data_adapters\\py_dataset_adapter.py:121: UserWarning: Your `PyDataset` class should call `super().__init__(**kwargs)` in its constructor. `**kwargs` can include `workers`, `use_multiprocessing`, `max_queue_size`. Do not pass these arguments to `fit()`, as they will be ignored.\n",
      "  self._warn_if_super_not_called()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "\u001b[1m108/108\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m33s\u001b[0m 302ms/step - accuracy: 0.3532 - loss: 1.5132 - val_accuracy: 0.4832 - val_loss: 1.2771\n",
      "Epoch 2/20\n",
      "\u001b[1m  1/108\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m19s\u001b[0m 181ms/step - accuracy: 0.5000 - loss: 1.2318"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Seán\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\keras\\src\\trainers\\epoch_iterator.py:107: UserWarning: Your input ran out of data; interrupting training. Make sure that your dataset or generator can generate at least `steps_per_epoch * epochs` batches. You may need to use the `.repeat()` function when building your dataset.\n",
      "  self._interrupted_warning()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m108/108\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 11ms/step - accuracy: 0.5000 - loss: 1.2318 - val_accuracy: 0.4736 - val_loss: 1.2862\n",
      "Epoch 3/20\n",
      "\u001b[1m108/108\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 193ms/step - accuracy: 0.5450 - loss: 1.1380 - val_accuracy: 0.5637 - val_loss: 1.1183\n",
      "Epoch 4/20\n",
      "\u001b[1m108/108\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 11ms/step - accuracy: 0.5938 - loss: 0.9472 - val_accuracy: 0.5553 - val_loss: 1.1139\n",
      "Epoch 5/20\n",
      "\u001b[1m108/108\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 190ms/step - accuracy: 0.6410 - loss: 0.9439 - val_accuracy: 0.5769 - val_loss: 1.0795\n",
      "Epoch 6/20\n",
      "\u001b[1m108/108\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 11ms/step - accuracy: 0.8438 - loss: 0.3735 - val_accuracy: 0.5877 - val_loss: 1.0385\n",
      "Epoch 7/20\n",
      "\u001b[1m108/108\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 191ms/step - accuracy: 0.7325 - loss: 0.7133 - val_accuracy: 0.6094 - val_loss: 1.0666\n",
      "Epoch 8/20\n",
      "\u001b[1m108/108\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 11ms/step - accuracy: 0.7188 - loss: 0.5935 - val_accuracy: 0.5950 - val_loss: 1.1182\n",
      "Epoch 9/20\n",
      "\u001b[1m108/108\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 189ms/step - accuracy: 0.8285 - loss: 0.4752 - val_accuracy: 0.6130 - val_loss: 1.2346\n",
      "Epoch 10/20\n",
      "\u001b[1m108/108\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 11ms/step - accuracy: 0.9062 - loss: 0.3792 - val_accuracy: 0.6226 - val_loss: 1.2266\n",
      "Epoch 11/20\n",
      "\u001b[1m108/108\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 190ms/step - accuracy: 0.9174 - loss: 0.2652 - val_accuracy: 0.5877 - val_loss: 1.7047\n",
      "Epoch 12/20\n",
      "\u001b[1m108/108\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 11ms/step - accuracy: 0.9375 - loss: 0.1841 - val_accuracy: 0.5889 - val_loss: 1.6834\n",
      "Epoch 13/20\n",
      "\u001b[1m108/108\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 191ms/step - accuracy: 0.9673 - loss: 0.1244 - val_accuracy: 0.5697 - val_loss: 1.8618\n",
      "Epoch 14/20\n",
      "\u001b[1m108/108\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 11ms/step - accuracy: 0.9375 - loss: 0.1045 - val_accuracy: 0.5865 - val_loss: 1.7824\n",
      "Epoch 15/20\n",
      "\u001b[1m108/108\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 187ms/step - accuracy: 0.9748 - loss: 0.0902 - val_accuracy: 0.5889 - val_loss: 1.9585\n",
      "Epoch 16/20\n",
      "\u001b[1m108/108\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - accuracy: 0.9375 - loss: 0.2645 - val_accuracy: 0.5829 - val_loss: 1.9830\n",
      "Epoch 17/20\n",
      "\u001b[1m108/108\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 180ms/step - accuracy: 0.9870 - loss: 0.0633 - val_accuracy: 0.5962 - val_loss: 2.3282\n",
      "Epoch 18/20\n",
      "\u001b[1m108/108\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - accuracy: 1.0000 - loss: 0.0183 - val_accuracy: 0.5938 - val_loss: 2.3692\n",
      "Epoch 19/20\n",
      "\u001b[1m108/108\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 180ms/step - accuracy: 0.9966 - loss: 0.0239 - val_accuracy: 0.6022 - val_loss: 2.3787\n",
      "Epoch 20/20\n",
      "\u001b[1m108/108\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 11ms/step - accuracy: 1.0000 - loss: 0.0079 - val_accuracy: 0.5974 - val_loss: 2.4190\n"
     ]
    }
   ],
   "source": [
    "# Train model\n",
    "\n",
    "history = model.fit(\n",
    "    train_generator,\n",
    "    steps_per_epoch=train_generator.samples // batch_size,\n",
    "    validation_data=validation_generator,\n",
    "    validation_steps=validation_generator.samples // batch_size,\n",
    "    epochs=epochs\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 42ms/step - accuracy: 0.5968 - loss: 2.3820\n",
      "Validation loss: 2.3857035636901855\n",
      "Validation accuracy: 0.6011627912521362\n"
     ]
    }
   ],
   "source": [
    "# Analyse loss and accuracy measurements\n",
    "\n",
    "loss, accuracy = model.evaluate(validation_generator)\n",
    "print(f'Validation loss: {loss}')\n",
    "print(f'Validation accuracy: {accuracy}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Results\n",
    "\n",
    "An accuracy value of roughly 0.60 tells us that the model is correct more often than not, but not accurate enough to be relied upon consistently. Using different batch sizes and epoch values resulted in little to no change."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 58ms/step\n",
      "Predicted class: [3]\n"
     ]
    }
   ],
   "source": [
    "# Test model on image of sunflower\n",
    "\n",
    "img_path = 'flowers/sunflower/6953297_8576bf4ea3.jpg'\n",
    "img = image.load_img(img_path, target_size=target_size)\n",
    "img_array = image.img_to_array(img) / 255.0\n",
    "img_array = np.expand_dims(img_array, axis=0)\n",
    "predictions = model.predict(img_array)\n",
    "predicted_class = np.argmax(predictions, axis=1)\n",
    "print(f'Predicted class: {predicted_class}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step\n",
      "Predicted class: [4]\n"
     ]
    }
   ],
   "source": [
    "# Test model on image of daisy\n",
    "\n",
    "img_path = 'flowers/daisy/144603918_b9de002f60_m.jpg'\n",
    "img = image.load_img(img_path, target_size=target_size)\n",
    "img_array = image.img_to_array(img) / 255.0\n",
    "img_array = np.expand_dims(img_array, axis=0)\n",
    "predictions = model.predict(img_array)\n",
    "predicted_class = np.argmax(predictions, axis=1)\n",
    "print(f'Predicted class: {predicted_class}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
