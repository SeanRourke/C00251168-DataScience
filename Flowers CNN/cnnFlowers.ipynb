{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Flower Classification Convolutional Neural Network Project\n",
    "\n",
    "### Dataset\n",
    "\n",
    "Dataset containing 4,242 images of flowers. Dataset obtained from Kaggle at https://www.kaggle.com/datasets/alxmamaev/flowers-recognition. \n",
    "\n",
    "Dataset contained following (class / number of images):\n",
    "\n",
    "daisy / 764\n",
    "dandelion / 1,052\n",
    "rose / 784\n",
    "sunflower / 733\n",
    "tulip / 984\n",
    "\n",
    "##### Preprocessing\n",
    "\n",
    "All images are resized to 244 x 244 in this notebook.\n",
    "\n",
    "### Findings\n",
    "\n",
    "From performing operations on this dataset, I have found that the first convolutional neural network created to classify the flowers was not accurate. As seen by the accuracy score of 0.63, the model is correct in its classification more often than not, but is not consistent enough to be relied upon."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Changelog\n",
    "\n",
    "#### Version 1\n",
    "\n",
    "- Set root directory for image locations\n",
    "- Read in images.\n",
    "- Resized images to all be the same size.\n",
    "- Create ImageDataGenerator.\n",
    "- Split data into training and validation data.\n",
    "- Created  and compiled model.\n",
    "- Trained model on the data.\n",
    "- Analysed accuracy of the model using accuracy and loss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import all necessary libraries\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "import cv2\n",
    "import os\n",
    "from tensorflow.keras import layers, models\n",
    "import numpy as np\n",
    "from tensorflow.keras.preprocessing import image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set root directory for class folders and target size for images\n",
    "\n",
    "root_directory = 'flowers/'\n",
    "target_size = (224, 224)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Resize images in dataset to target_size\n",
    "\n",
    "for class_folder in os.listdir(root_directory):\n",
    "    class_path = os.path.join(root_directory, class_folder)\n",
    "    if os.path.isdir(class_path):  # Check if it is a directory\n",
    "        # Loop through all images in the class folder\n",
    "        for filename in os.listdir(class_path):\n",
    "            if filename.endswith('.jpg') or filename.endswith('.png'):\n",
    "                img_path = os.path.join(class_path, filename)\n",
    "                img = cv2.imread(img_path)\n",
    "                img_resized = cv2.resize(img, target_size)\n",
    "                cv2.imwrite(img_path, img_resized)  # Overwrite the original image or save to a new file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set number of training samples used in one iteration of training\n",
    "# Set number of passes through dataset\n",
    "\n",
    "batch_size = 32\n",
    "epochs = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create ImageDataGenerator to allow real-time data augmentation. This helps the model generalise and reduces overfitting\n",
    "# Rescale pixel values from 0-255 to 0-1 to improve convergence during training\n",
    "# Reserve 20% of the dataset for validation\n",
    "\n",
    "train_datagen = ImageDataGenerator(rescale=1./255, validation_split=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 3457 images belonging to 5 classes.\n"
     ]
    }
   ],
   "source": [
    "# Load training data\n",
    "\n",
    "train_generator = train_datagen.flow_from_directory(\n",
    "    root_directory,\n",
    "    target_size=target_size,\n",
    "    batch_size=batch_size,\n",
    "    class_mode='categorical',\n",
    "    subset='training'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'daisy': 0, 'dandelion': 1, 'rose': 2, 'sunflower': 3, 'tulip': 4}\n"
     ]
    }
   ],
   "source": [
    "# Check class indice values\n",
    "\n",
    "print(train_generator.class_indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 860 images belonging to 5 classes.\n"
     ]
    }
   ],
   "source": [
    "# Load validation data\n",
    "\n",
    "validation_generator = train_datagen.flow_from_directory(\n",
    "    root_directory,\n",
    "    target_size=target_size,\n",
    "    batch_size=batch_size,\n",
    "    class_mode='categorical',\n",
    "    subset='validation'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Seán\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\keras\\src\\layers\\convolutional\\base_conv.py:107: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    }
   ],
   "source": [
    "# Three 2D convolution layers for RGB colours, filter size of 3x3\n",
    "# Reduce height and width of feature maps to reduce number of parameters and computation in the network\n",
    "\n",
    "model = models.Sequential([\n",
    "    layers.Conv2D(32, (3, 3), activation='relu', input_shape=(224, 224, 3)),\n",
    "    layers.MaxPooling2D(pool_size=(2, 2)),\n",
    "    layers.Conv2D(64, (3, 3), activation='relu'),\n",
    "    layers.MaxPooling2D(pool_size=(2, 2)),\n",
    "    layers.Conv2D(128, (3, 3), activation='relu'),\n",
    "    layers.MaxPooling2D(pool_size=(2, 2)),\n",
    "    layers.Flatten(),   # flatten 3D output to 1D vector for following layers\n",
    "    layers.Dense(128, activation='relu'),   # Learn high-level features by combining features learning in convolutional layers\n",
    "    layers.Dense(len(train_generator.class_indices), activation='softmax')  # Same number of neurons as classes, softmax outputs probability for each class\n",
    "]);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adaptive Moment Estimation optimiser because it is memory-efficient and adapts the learning rate dynamically for each parameter\n",
    "# Minimise loss function during training\n",
    "\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Seán\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\keras\\src\\trainers\\data_adapters\\py_dataset_adapter.py:121: UserWarning: Your `PyDataset` class should call `super().__init__(**kwargs)` in its constructor. `**kwargs` can include `workers`, `use_multiprocessing`, `max_queue_size`. Do not pass these arguments to `fit()`, as they will be ignored.\n",
      "  self._warn_if_super_not_called()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "\u001b[1m108/108\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m31s\u001b[0m 281ms/step - accuracy: 0.3605 - loss: 1.7730 - val_accuracy: 0.5529 - val_loss: 1.0848\n",
      "Epoch 2/10\n",
      "\u001b[1m  1/108\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m17s\u001b[0m 162ms/step - accuracy: 0.4062 - loss: 1.1524"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Seán\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\keras\\src\\trainers\\epoch_iterator.py:107: UserWarning: Your input ran out of data; interrupting training. Make sure that your dataset or generator can generate at least `steps_per_epoch * epochs` batches. You may need to use the `.repeat()` function when building your dataset.\n",
      "  self._interrupted_warning()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m108/108\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 11ms/step - accuracy: 0.4062 - loss: 1.1524 - val_accuracy: 0.5505 - val_loss: 1.0748\n",
      "Epoch 3/10\n",
      "\u001b[1m108/108\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 173ms/step - accuracy: 0.6257 - loss: 0.9940 - val_accuracy: 0.6382 - val_loss: 0.9523\n",
      "Epoch 4/10\n",
      "\u001b[1m108/108\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - accuracy: 0.5625 - loss: 0.9802 - val_accuracy: 0.6118 - val_loss: 0.9867\n",
      "Epoch 5/10\n",
      "\u001b[1m108/108\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 170ms/step - accuracy: 0.6842 - loss: 0.7996 - val_accuracy: 0.6310 - val_loss: 0.9934\n",
      "Epoch 6/10\n",
      "\u001b[1m108/108\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - accuracy: 0.7188 - loss: 0.5296 - val_accuracy: 0.6358 - val_loss: 0.9976\n",
      "Epoch 7/10\n",
      "\u001b[1m108/108\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 168ms/step - accuracy: 0.8223 - loss: 0.4854 - val_accuracy: 0.6142 - val_loss: 1.0347\n",
      "Epoch 8/10\n",
      "\u001b[1m108/108\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - accuracy: 0.8438 - loss: 0.5048 - val_accuracy: 0.6238 - val_loss: 1.0326\n",
      "Epoch 9/10\n",
      "\u001b[1m108/108\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 169ms/step - accuracy: 0.9037 - loss: 0.2722 - val_accuracy: 0.6238 - val_loss: 1.3882\n",
      "Epoch 10/10\n",
      "\u001b[1m108/108\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - accuracy: 0.8750 - loss: 0.2910 - val_accuracy: 0.6250 - val_loss: 1.3644\n"
     ]
    }
   ],
   "source": [
    "# Train model\n",
    "\n",
    "history = model.fit(\n",
    "    train_generator,\n",
    "    steps_per_epoch=train_generator.samples // batch_size,\n",
    "    validation_data=validation_generator,\n",
    "    validation_steps=validation_generator.samples // batch_size,\n",
    "    epochs=epochs\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 44ms/step - accuracy: 0.6370 - loss: 1.3624\n",
      "Validation loss: 1.367043137550354\n",
      "Validation accuracy: 0.6255813837051392\n"
     ]
    }
   ],
   "source": [
    "# Analyse loss and accuracy measurements\n",
    "\n",
    "loss, accuracy = model.evaluate(validation_generator)\n",
    "print(f'Validation loss: {loss}')\n",
    "print(f'Validation accuracy: {accuracy}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Results\n",
    "\n",
    "An accuracy value of 0.63 tells us that the model is correct more often than not, but not accurate enough to be relied upon consistently."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step\n",
      "Predicted class: [3]\n"
     ]
    }
   ],
   "source": [
    "# Test model on image of sunflower\n",
    "\n",
    "img_path = 'flowers/sunflower/6953297_8576bf4ea3.jpg'\n",
    "img = image.load_img(img_path, target_size=target_size)\n",
    "img_array = image.img_to_array(img) / 255.0\n",
    "img_array = np.expand_dims(img_array, axis=0)\n",
    "predictions = model.predict(img_array)\n",
    "predicted_class = np.argmax(predictions, axis=1)\n",
    "print(f'Predicted class: {predicted_class}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step\n",
      "Predicted class: [4]\n"
     ]
    }
   ],
   "source": [
    "# Test model on image of daisy\n",
    "\n",
    "img_path = 'flowers/daisy/144603918_b9de002f60_m.jpg'\n",
    "img = image.load_img(img_path, target_size=target_size)\n",
    "img_array = image.img_to_array(img) / 255.0\n",
    "img_array = np.expand_dims(img_array, axis=0)\n",
    "predictions = model.predict(img_array)\n",
    "predicted_class = np.argmax(predictions, axis=1)\n",
    "print(f'Predicted class: {predicted_class}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
